{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q git+https://github.com/sanchit-gandhi/whisper-jax.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'ai_video_editor_sample_cropped_file.flac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'audio_2.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_jax import FlaxWhisperPipline\n",
    "import jax.numpy as jnp\n",
    "\n",
    "pipeline_tiny = FlaxWhisperPipline(\"openai/whisper-tiny\", dtype=jnp.bfloat16, batch_size=16, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe and return timestamps\n",
    "outputs_tiny = pipeline_tiny(fileName,  task=\"transcribe\", return_timestamps=True, chunk_length_s=30.0, language=\"en\")\n",
    "text = outputs_tiny[\"text\"]  # transcription\n",
    "chunks = outputs_tiny[\"chunks\"]  # transcription + timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_chunks = json.dumps(chunks, indent=2)\n",
    "with open('tiny.json', 'w') as json_file:\n",
    "    json.dump(chunks, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_base = FlaxWhisperPipline(\"openai/whisper-base\", dtype=jnp.bfloat16, batch_size=16)\n",
    "\n",
    "\n",
    "# transcribe and return timestamps\n",
    "outputs_base = pipeline_base(fileName,  task=\"transcribe\", return_timestamps=True)\n",
    "chunks_base = outputs_base[\"chunks\"]  # transcription + timestamps\n",
    "\n",
    "\n",
    "json_chunks = json.dumps(chunks, indent=2)\n",
    "with open('base.json', 'w') as json_file:\n",
    "    json.dump(chunks, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_small = FlaxWhisperPipline(\"openai/whisper-small\", dtype=jnp.bfloat16, batch_size=16)\n",
    "\n",
    "\n",
    "# transcribe and return timestamps\n",
    "outputs_small = pipeline_small(fileName,  task=\"transcribe\", return_timestamps=True)\n",
    "chunks_small = outputs_small[\"chunks\"]  # transcription + timestamps\n",
    "\n",
    "\n",
    "json_chunks = json.dumps(chunks, indent=2)\n",
    "with open('small.json', 'w') as json_file:\n",
    "    json.dump(chunks, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418a209c56394f98bcf97600dd21ddfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading flax_model.msgpack:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069ae9748afd445391745ca462d44f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_medium = FlaxWhisperPipline(\"openai/whisper-medium\", dtype=jnp.bfloat16, batch_size=16)\n",
    "\n",
    "\n",
    "# transcribe and return timestamps\n",
    "outputs_medium = pipeline_medium(fileName,  task=\"transcribe\", return_timestamps=True)\n",
    "chunks_medium = outputs_medium[\"chunks\"]  # transcription + timestamps\n",
    "\n",
    "\n",
    "json_chunks = json.dumps(chunks, indent=2)\n",
    "with open('medium.json', 'w') as json_file:\n",
    "    json.dump(chunks, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-14 12:11:08.668] [ctranslate2] [thread 2613137] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"base\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(fileName, beam_size=1, word_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 4.88s]  on some. I'm certainly not colored-moan, but I'm hoping it meets our needs and I know\n",
      "[4.88s -> 7.10s]  that the executive is hoping that as well.\n",
      "[7.56s -> 11.34s]  Is there anything else that stands out to you or anything else you want to touch on from\n",
      "[11.34s -> 12.18s]  the last four years?\n",
      "[14.06s -> 20.46s]  You know, the executive understands our retention issues with support staff and APAs, so we're\n",
      "[20.46s -> 28.16s]  working on that as we speak. So the retention issues, it's being able to, we want career\n",
      "[28.16s -> 32.86s]  prosecutors, we want, we eventually want to have a staff, not one. We eventually need\n",
      "[32.86s -> 37.24s]  to have a staff. There's representative of what this county is and in the executive\n",
      "[37.24s -> 41.18s]  knows, and I think everybody knows, I've been talking about it for years, that when it comes\n",
      "[41.18s -> 45.28s]  to kind of our size, we are, we are again severely understaffed, but we're getting there,\n",
      "[45.28s -> 49.74s]  we're working toward that goal. And as long as I see some progress, then I'm happy with\n",
      "[49.74s -> 55.16s]  the leadership that we're having. And again, he gets what we need and when I call him, he picks\n",
      "[55.16s -> 60.48s]  up the phone and returns my calls. And so, and then addresses those concerns. And so I would like\n",
      "[60.48s -> 65.06s]  to think that he feels the same way about this office, that I will address the concerns that\n",
      "[65.06s -> 69.74s]  that he has. And I'm very pleased with those, with those things that have happened so far.\n",
      "[70.02s -> 74.60s]  And what kind of have people wrap up just kind of give their assessment where Wayne County is today?\n",
      "[74.80s -> 78.82s]  If you want to say Wayne County is and then fill in kind of whatever you want to say, where we are\n",
      "[78.82s -> 83.52s]  on the trajectory or the turnaround, whatever comes to mind. It sounds so cliche, it really does.\n",
      "[83.52s -> 91.08s]  But Wayne County is a place where I'm raising my children, where my office is, where our\n",
      "[91.08s -> 98.16s]  constituencies, constituencies is, we want it to be safe first and foremost, my overriding concern\n",
      "[98.88s -> 103.42s]  for not only myself, not only for my family, not only for my staff, but for the community that we\n",
      "[103.42s -> 110.54s]  serve is that we can be safe, a progressive place, a place that's not afraid to address any issues\n",
      "[110.54s -> 114.90s]  in the criminal justice system. And at the end of the day, it's a safe place to live work\n",
      "[115.88s -> 121.46s]  worship and live. And one last thing I forget, I should probably say that it's a safe place to\n",
      "[122.38s -> 129.00s]  live, work, worship and play. And the one last thing I would say is I remember you and I came on board\n",
      "[129.00s -> 132.58s]  about two and a half years ago and I remember looking back at what I've done in the first year or so\n",
      "[132.58s -> 136.88s]  as I'm trying to get up the speed, I remember and his first budget he was able to give a, you know,\n",
      "[136.88s -> 144.20s]  car about a million dollars for the backlog of the red kids. That was that was it for that to occur\n",
      "[144.20s -> 148.86s]  at a time when you're making a lot of difficult financial decisions. That that,\n",
      "[150.22s -> 157.36s]  when I first met with the CEO about when he was declaring his candidacy and I met with him to\n",
      "[157.36s -> 162.30s]  talk about whether I was going to support him or not. And there were several things that were\n",
      "[162.30s -> 169.86s]  deal-breakers for me. And I knew that I had to have some support for the main issues of the office.\n",
      "[170.58s -> 175.06s]  One of those main issues was we had no previous support from the previous administration for\n",
      "[175.06s -> 180.16s]  the sexual assault kids that had been discovered in 2009. The others were, things I've talked about\n",
      "[180.16s -> 180.98s]  already, they were\n"
     ]
    }
   ],
   "source": [
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/metadesign/Desktop/projects/learning/transcribe-python/whisper-jax.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/metadesign/Desktop/projects/learning/transcribe-python/whisper-jax.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m json_chunks \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(segments, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/metadesign/Desktop/projects/learning/transcribe-python/whisper-jax.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msegments.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/metadesign/Desktop/projects/learning/transcribe-python/whisper-jax.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(json_chunks, json_file, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "json_chunks = json.dumps(segments, indent=2)\n",
    "with open('segments.json', 'w') as json_file:\n",
    "    json.dump(json_chunks, json_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
